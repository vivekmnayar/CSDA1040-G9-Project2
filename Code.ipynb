{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load ascii text and covert to lowercase\n",
    "filename = \"wonderland.txt\"\n",
    "raw_text = open(filename, 'r', encoding='utf-8').read()\n",
    "raw_text = raw_text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create mapping of unique chars to integers\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n': 0,\n",
       " ' ': 1,\n",
       " '!': 2,\n",
       " '\"': 3,\n",
       " \"'\": 4,\n",
       " '(': 5,\n",
       " ')': 6,\n",
       " '*': 7,\n",
       " ',': 8,\n",
       " '-': 9,\n",
       " '.': 10,\n",
       " '0': 11,\n",
       " '3': 12,\n",
       " ':': 13,\n",
       " ';': 14,\n",
       " '?': 15,\n",
       " '[': 16,\n",
       " ']': 17,\n",
       " '_': 18,\n",
       " 'a': 19,\n",
       " 'b': 20,\n",
       " 'c': 21,\n",
       " 'd': 22,\n",
       " 'e': 23,\n",
       " 'f': 24,\n",
       " 'g': 25,\n",
       " 'h': 26,\n",
       " 'i': 27,\n",
       " 'j': 28,\n",
       " 'k': 29,\n",
       " 'l': 30,\n",
       " 'm': 31,\n",
       " 'n': 32,\n",
       " 'o': 33,\n",
       " 'p': 34,\n",
       " 'q': 35,\n",
       " 'r': 36,\n",
       " 's': 37,\n",
       " 't': 38,\n",
       " 'u': 39,\n",
       " 'v': 40,\n",
       " 'w': 41,\n",
       " 'x': 42,\n",
       " 'y': 43,\n",
       " 'z': 44}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_to_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  144408\n",
      "Total Vocab:  45\n"
     ]
    }
   ],
   "source": [
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print (\"Total Characters: \", n_chars)\n",
    "print (\"Total Vocab: \", n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  144308\n"
     ]
    }
   ],
   "source": [
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "\tseq_in = raw_text[i:i + seq_length]\n",
    "\tseq_out = raw_text[i + seq_length]\n",
    "\tdataX.append([char_to_int[char] for char in seq_in])\n",
    "\tdataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print (\"Total Patterns: \", n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1128/1128 [==============================] - ETA: 0s - loss: 2.9652\n",
      "Epoch 00001: loss improved from inf to 2.96521, saving model to weights-improvement-01-2.9652.hdf5\n",
      "1128/1128 [==============================] - 176s 156ms/step - loss: 2.9652\n",
      "Epoch 2/20\n",
      "1128/1128 [==============================] - ETA: 0s - loss: 2.7684\n",
      "Epoch 00002: loss improved from 2.96521 to 2.76839, saving model to weights-improvement-02-2.7684.hdf5\n",
      "1128/1128 [==============================] - 188s 167ms/step - loss: 2.7684\n",
      "Epoch 3/20\n",
      "1128/1128 [==============================] - ETA: 0s - loss: 2.6597\n",
      "Epoch 00003: loss improved from 2.76839 to 2.65972, saving model to weights-improvement-03-2.6597.hdf5\n",
      "1128/1128 [==============================] - 203s 180ms/step - loss: 2.6597\n",
      "Epoch 4/20\n",
      "1128/1128 [==============================] - ETA: 0s - loss: 2.5771\n",
      "Epoch 00004: loss improved from 2.65972 to 2.57710, saving model to weights-improvement-04-2.5771.hdf5\n",
      "1128/1128 [==============================] - 215s 190ms/step - loss: 2.5771\n",
      "Epoch 5/20\n",
      "1128/1128 [==============================] - ETA: 0s - loss: 2.5124\n",
      "Epoch 00005: loss improved from 2.57710 to 2.51242, saving model to weights-improvement-05-2.5124.hdf5\n",
      "1128/1128 [==============================] - 212s 188ms/step - loss: 2.5124\n",
      "Epoch 6/20\n",
      "1128/1128 [==============================] - ETA: 0s - loss: 2.4541\n",
      "Epoch 00006: loss improved from 2.51242 to 2.45410, saving model to weights-improvement-06-2.4541.hdf5\n",
      "1128/1128 [==============================] - 209s 186ms/step - loss: 2.4541\n",
      "Epoch 7/20\n",
      "1128/1128 [==============================] - ETA: 0s - loss: 2.3987\n",
      "Epoch 00007: loss improved from 2.45410 to 2.39865, saving model to weights-improvement-07-2.3987.hdf5\n",
      "1128/1128 [==============================] - 215s 191ms/step - loss: 2.3987\n",
      "Epoch 8/20\n",
      "1128/1128 [==============================] - ETA: 0s - loss: 2.3498\n",
      "Epoch 00008: loss improved from 2.39865 to 2.34975, saving model to weights-improvement-08-2.3498.hdf5\n",
      "1128/1128 [==============================] - 207s 183ms/step - loss: 2.3498\n",
      "Epoch 9/20\n",
      "1128/1128 [==============================] - ETA: 0s - loss: 2.3027\n",
      "Epoch 00009: loss improved from 2.34975 to 2.30268, saving model to weights-improvement-09-2.3027.hdf5\n",
      "1128/1128 [==============================] - 208s 185ms/step - loss: 2.3027\n",
      "Epoch 10/20\n",
      "1128/1128 [==============================] - ETA: 0s - loss: 2.2563\n",
      "Epoch 00010: loss improved from 2.30268 to 2.25630, saving model to weights-improvement-10-2.2563.hdf5\n",
      "1128/1128 [==============================] - 213s 189ms/step - loss: 2.2563\n",
      "Epoch 11/20\n",
      "1128/1128 [==============================] - ETA: 0s - loss: 2.2177\n",
      "Epoch 00011: loss improved from 2.25630 to 2.21775, saving model to weights-improvement-11-2.2177.hdf5\n",
      "1128/1128 [==============================] - 209s 185ms/step - loss: 2.2177\n",
      "Epoch 12/20\n",
      "1128/1128 [==============================] - ETA: 0s - loss: 2.7781\n",
      "Epoch 00012: loss did not improve from 2.21775\n",
      "1128/1128 [==============================] - 219s 194ms/step - loss: 2.7781\n",
      "Epoch 13/20\n",
      "1128/1128 [==============================] - ETA: 0s - loss: 2.3385\n",
      "Epoch 00013: loss did not improve from 2.21775\n",
      "1128/1128 [==============================] - 218s 193ms/step - loss: 2.3385\n",
      "Epoch 14/20\n",
      "1128/1128 [==============================] - ETA: 0s - loss: 2.2200\n",
      "Epoch 00014: loss did not improve from 2.21775\n",
      "1128/1128 [==============================] - 213s 189ms/step - loss: 2.2200\n",
      "Epoch 15/20\n",
      "1128/1128 [==============================] - ETA: 0s - loss: 2.1736\n",
      "Epoch 00015: loss improved from 2.21775 to 2.17355, saving model to weights-improvement-15-2.1736.hdf5\n",
      "1128/1128 [==============================] - 216s 192ms/step - loss: 2.1736\n",
      "Epoch 16/20\n",
      "1128/1128 [==============================] - ETA: 0s - loss: 2.1361\n",
      "Epoch 00016: loss improved from 2.17355 to 2.13607, saving model to weights-improvement-16-2.1361.hdf5\n",
      "1128/1128 [==============================] - 221s 196ms/step - loss: 2.1361\n",
      "Epoch 17/20\n",
      "1128/1128 [==============================] - ETA: 0s - loss: 2.1068\n",
      "Epoch 00017: loss improved from 2.13607 to 2.10680, saving model to weights-improvement-17-2.1068.hdf5\n",
      "1128/1128 [==============================] - 218s 193ms/step - loss: 2.1068\n",
      "Epoch 18/20\n",
      "1128/1128 [==============================] - ETA: 0s - loss: 2.0736\n",
      "Epoch 00018: loss improved from 2.10680 to 2.07364, saving model to weights-improvement-18-2.0736.hdf5\n",
      "1128/1128 [==============================] - 219s 194ms/step - loss: 2.0736\n",
      "Epoch 19/20\n",
      "1128/1128 [==============================] - ETA: 0s - loss: 2.0429\n",
      "Epoch 00019: loss improved from 2.07364 to 2.04288, saving model to weights-improvement-19-2.0429.hdf5\n",
      "1128/1128 [==============================] - 229s 203ms/step - loss: 2.0429\n",
      "Epoch 20/20\n",
      "1128/1128 [==============================] - ETA: 0s - loss: 2.0158\n",
      "Epoch 00020: loss improved from 2.04288 to 2.01577, saving model to weights-improvement-20-2.0158.hdf5\n",
      "1128/1128 [==============================] - 225s 199ms/step - loss: 2.0158\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f82488dbb90>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=20, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the network weights\n",
    "filename = \"weights-improvement-20-2.0158.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_to_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\" ive it up,' alice replied: 'what's the answer?'\n",
      "\n",
      "'i haven't the slightest idea,' said the hatter.\n",
      "\n",
      "' \"\n",
      "ieme you mene the dormouse ' said the caterpillar.\n",
      "\n",
      "'ie you d leter see toieteing ' said the manch hare.\n",
      "\n",
      "'ieme you mene toe bene ' said the caterpillar.\n",
      "\n",
      "'ie you d leter seen tou do a pirtle to tot,' said the caterpillar.\n",
      "\n",
      "'ie you d leter see toieteing ' said the manch hare.\n",
      "\n",
      "'ieme you mene toe bene ' said the caterpillar.\n",
      "\n",
      "'ie you d leter seen tou do a pirtle to tot,' said the caterpillar.\n",
      "\n",
      "'ie you d leter see toieteing ' said the manch hare.\n",
      "\n",
      "'ieme you mene toe bene ' said the caterpillar.\n",
      "\n",
      "'ie you d leter seen tou do a pirtle to tot,' said the caterpillar.\n",
      "\n",
      "'ie you d leter see toieteing ' said the manch hare.\n",
      "\n",
      "'ieme you mene toe bene ' said the caterpillar.\n",
      "\n",
      "'ie you d leter seen tou do a pirtle to tot,' said the caterpillar.\n",
      "\n",
      "'ie you d leter see toieteing ' said the manch hare.\n",
      "\n",
      "'ieme you mene toe bene ' said the caterpillar.\n",
      "\n",
      "'ie you d leter seen tou do a pirtle to tot,' said the caterpillar.\n",
      "\n",
      "'ie you d leter see toieteing ' said the manch hare.\n",
      "\n",
      "'ieme you mene toe bene ' said the\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# pick a random seed\n",
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print (\"Seed:\")\n",
    "print (\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "# generate characters\n",
    "for i in range(1000):\n",
    "\tx = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "\tx = x / float(n_vocab)\n",
    "\tprediction = model.predict(x, verbose=0)\n",
    "\tindex = numpy.argmax(prediction)\n",
    "\tresult = int_to_char[index]\n",
    "\tseq_in = [int_to_char[value] for value in pattern]\n",
    "\tsys.stdout.write(result)\n",
    "\tpattern.append(index)\n",
    "\tpattern = pattern[1:len(pattern)]\n",
    "print (\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###############\n",
    "## Larger LSTM Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  144408\n",
      "Total Vocab:  45\n",
      "Total Patterns:  144308\n",
      "Epoch 1/50\n",
      "2255/2255 [==============================] - ETA: 0s - loss: 2.7894\n",
      "Epoch 00001: loss improved from inf to 2.78942, saving model to weights-improvement-01-2.7894-bigger.hdf5\n",
      "2255/2255 [==============================] - 634s 281ms/step - loss: 2.7894\n",
      "Epoch 2/50\n",
      "2255/2255 [==============================] - ETA: 0s - loss: 2.4172\n",
      "Epoch 00002: loss improved from 2.78942 to 2.41716, saving model to weights-improvement-02-2.4172-bigger.hdf5\n",
      "2255/2255 [==============================] - 629s 279ms/step - loss: 2.4172\n",
      "Epoch 3/50\n",
      "2255/2255 [==============================] - ETA: 0s - loss: 2.2165\n",
      "Epoch 00003: loss improved from 2.41716 to 2.21646, saving model to weights-improvement-03-2.2165-bigger.hdf5\n",
      "2255/2255 [==============================] - 633s 281ms/step - loss: 2.2165\n",
      "Epoch 4/50\n",
      "2255/2255 [==============================] - ETA: 0s - loss: 2.0852\n",
      "Epoch 00004: loss improved from 2.21646 to 2.08522, saving model to weights-improvement-04-2.0852-bigger.hdf5\n",
      "2255/2255 [==============================] - 1429s 634ms/step - loss: 2.0852\n",
      "Epoch 5/50\n",
      "2255/2255 [==============================] - ETA: 0s - loss: 1.9824\n",
      "Epoch 00005: loss improved from 2.08522 to 1.98235, saving model to weights-improvement-05-1.9824-bigger.hdf5\n",
      "2255/2255 [==============================] - 631s 280ms/step - loss: 1.9824\n",
      "Epoch 6/50\n",
      "2255/2255 [==============================] - ETA: 0s - loss: 1.9079\n",
      "Epoch 00006: loss improved from 1.98235 to 1.90792, saving model to weights-improvement-06-1.9079-bigger.hdf5\n",
      "2255/2255 [==============================] - 7860s 3s/step - loss: 1.9079\n",
      "Epoch 7/50\n",
      "2255/2255 [==============================] - ETA: 0s - loss: 1.8423 \n",
      "Epoch 00007: loss improved from 1.90792 to 1.84231, saving model to weights-improvement-07-1.8423-bigger.hdf5\n",
      "2255/2255 [==============================] - 22575s 10s/step - loss: 1.8423\n",
      "Epoch 8/50\n",
      "2255/2255 [==============================] - ETA: 0s - loss: 1.7872\n",
      "Epoch 00008: loss improved from 1.84231 to 1.78722, saving model to weights-improvement-08-1.7872-bigger.hdf5\n",
      "2255/2255 [==============================] - 645s 286ms/step - loss: 1.7872\n",
      "Epoch 9/50\n",
      "2255/2255 [==============================] - ETA: 0s - loss: 1.7411\n",
      "Epoch 00009: loss improved from 1.78722 to 1.74113, saving model to weights-improvement-09-1.7411-bigger.hdf5\n",
      "2255/2255 [==============================] - 639s 283ms/step - loss: 1.7411\n",
      "Epoch 10/50\n",
      "2255/2255 [==============================] - ETA: 0s - loss: 1.6980\n",
      "Epoch 00010: loss improved from 1.74113 to 1.69801, saving model to weights-improvement-10-1.6980-bigger.hdf5\n",
      "2255/2255 [==============================] - 635s 281ms/step - loss: 1.6980\n",
      "Epoch 11/50\n",
      "2255/2255 [==============================] - ETA: 0s - loss: 1.6615\n",
      "Epoch 00011: loss improved from 1.69801 to 1.66148, saving model to weights-improvement-11-1.6615-bigger.hdf5\n",
      "2255/2255 [==============================] - 637s 282ms/step - loss: 1.6615\n",
      "Epoch 12/50\n",
      "2255/2255 [==============================] - ETA: 0s - loss: 1.6287\n",
      "Epoch 00012: loss improved from 1.66148 to 1.62874, saving model to weights-improvement-12-1.6287-bigger.hdf5\n",
      "2255/2255 [==============================] - 635s 282ms/step - loss: 1.6287\n",
      "Epoch 13/50\n",
      "2255/2255 [==============================] - ETA: 0s - loss: 1.5933\n",
      "Epoch 00013: loss improved from 1.62874 to 1.59329, saving model to weights-improvement-13-1.5933-bigger.hdf5\n",
      "2255/2255 [==============================] - 636s 282ms/step - loss: 1.5933\n",
      "Epoch 14/50\n",
      "2255/2255 [==============================] - ETA: 0s - loss: 1.5669\n",
      "Epoch 00014: loss improved from 1.59329 to 1.56687, saving model to weights-improvement-14-1.5669-bigger.hdf5\n",
      "2255/2255 [==============================] - 643s 285ms/step - loss: 1.5669\n",
      "Epoch 15/50\n",
      "2255/2255 [==============================] - ETA: 0s - loss: 1.5417\n",
      "Epoch 00015: loss improved from 1.56687 to 1.54168, saving model to weights-improvement-15-1.5417-bigger.hdf5\n",
      "2255/2255 [==============================] - 640s 284ms/step - loss: 1.5417\n",
      "Epoch 16/50\n",
      "2255/2255 [==============================] - ETA: 0s - loss: 1.5149\n",
      "Epoch 00016: loss improved from 1.54168 to 1.51495, saving model to weights-improvement-16-1.5149-bigger.hdf5\n",
      "2255/2255 [==============================] - 640s 284ms/step - loss: 1.5149\n",
      "Epoch 17/50\n",
      "2255/2255 [==============================] - ETA: 0s - loss: 1.4933\n",
      "Epoch 00017: loss improved from 1.51495 to 1.49326, saving model to weights-improvement-17-1.4933-bigger.hdf5\n",
      "2255/2255 [==============================] - 640s 284ms/step - loss: 1.4933\n",
      "Epoch 18/50\n",
      "2255/2255 [==============================] - ETA: 0s - loss: 1.4716\n",
      "Epoch 00018: loss improved from 1.49326 to 1.47163, saving model to weights-improvement-18-1.4716-bigger.hdf5\n",
      "2255/2255 [==============================] - 637s 282ms/step - loss: 1.4716\n",
      "Epoch 19/50\n",
      "2255/2255 [==============================] - ETA: 0s - loss: 1.4575\n",
      "Epoch 00019: loss improved from 1.47163 to 1.45748, saving model to weights-improvement-19-1.4575-bigger.hdf5\n",
      "2255/2255 [==============================] - 636s 282ms/step - loss: 1.4575\n",
      "Epoch 20/50\n",
      "2255/2255 [==============================] - ETA: 0s - loss: 1.4376\n",
      "Epoch 00020: loss improved from 1.45748 to 1.43756, saving model to weights-improvement-20-1.4376-bigger.hdf5\n",
      "2255/2255 [==============================] - 637s 282ms/step - loss: 1.4376\n",
      "Epoch 21/50\n",
      "2255/2255 [==============================] - ETA: 0s - loss: 1.4182\n",
      "Epoch 00021: loss improved from 1.43756 to 1.41824, saving model to weights-improvement-21-1.4182-bigger.hdf5\n",
      "2255/2255 [==============================] - 638s 283ms/step - loss: 1.4182\n",
      "Epoch 22/50\n",
      "2255/2255 [==============================] - ETA: 0s - loss: 1.4053\n",
      "Epoch 00022: loss improved from 1.41824 to 1.40534, saving model to weights-improvement-22-1.4053-bigger.hdf5\n",
      "2255/2255 [==============================] - 644s 286ms/step - loss: 1.4053\n",
      "Epoch 23/50\n",
      "2255/2255 [==============================] - ETA: 0s - loss: 1.3879\n",
      "Epoch 00023: loss improved from 1.40534 to 1.38792, saving model to weights-improvement-23-1.3879-bigger.hdf5\n",
      "2255/2255 [==============================] - 629s 279ms/step - loss: 1.3879\n",
      "Epoch 24/50\n",
      "2255/2255 [==============================] - ETA: 0s - loss: 1.3765\n",
      "Epoch 00024: loss improved from 1.38792 to 1.37647, saving model to weights-improvement-24-1.3765-bigger.hdf5\n",
      "2255/2255 [==============================] - 628s 278ms/step - loss: 1.3765\n",
      "Epoch 25/50\n",
      "2255/2255 [==============================] - ETA: 0s - loss: 1.3621\n",
      "Epoch 00025: loss improved from 1.37647 to 1.36209, saving model to weights-improvement-25-1.3621-bigger.hdf5\n",
      "2255/2255 [==============================] - 631s 280ms/step - loss: 1.3621\n",
      "Epoch 26/50\n",
      "2255/2255 [==============================] - ETA: 0s - loss: 1.3471\n",
      "Epoch 00026: loss improved from 1.36209 to 1.34714, saving model to weights-improvement-26-1.3471-bigger.hdf5\n",
      "2255/2255 [==============================] - 636s 282ms/step - loss: 1.3471\n",
      "Epoch 27/50\n",
      "2255/2255 [==============================] - ETA: 0s - loss: 1.3380\n",
      "Epoch 00027: loss improved from 1.34714 to 1.33795, saving model to weights-improvement-27-1.3380-bigger.hdf5\n",
      "2255/2255 [==============================] - 636s 282ms/step - loss: 1.3380\n",
      "Epoch 28/50\n",
      "2255/2255 [==============================] - ETA: 0s - loss: 1.3280\n",
      "Epoch 00028: loss improved from 1.33795 to 1.32798, saving model to weights-improvement-28-1.3280-bigger.hdf5\n",
      "2255/2255 [==============================] - 638s 283ms/step - loss: 1.3280\n",
      "Epoch 29/50\n",
      "2255/2255 [==============================] - ETA: 0s - loss: 1.3175\n",
      "Epoch 00029: loss improved from 1.32798 to 1.31754, saving model to weights-improvement-29-1.3175-bigger.hdf5\n",
      "2255/2255 [==============================] - 642s 285ms/step - loss: 1.3175\n",
      "Epoch 30/50\n",
      "2255/2255 [==============================] - ETA: 0s - loss: 1.3085\n",
      "Epoch 00030: loss improved from 1.31754 to 1.30849, saving model to weights-improvement-30-1.3085-bigger.hdf5\n",
      "2255/2255 [==============================] - 643s 285ms/step - loss: 1.3085\n",
      "Epoch 31/50\n",
      "2255/2255 [==============================] - ETA: 0s - loss: 1.2986\n",
      "Epoch 00031: loss improved from 1.30849 to 1.29864, saving model to weights-improvement-31-1.2986-bigger.hdf5\n",
      "2255/2255 [==============================] - 638s 283ms/step - loss: 1.2986\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/50\n",
      "2255/2255 [==============================] - ETA: 0s - loss: 1.2915\n",
      "Epoch 00032: loss improved from 1.29864 to 1.29148, saving model to weights-improvement-32-1.2915-bigger.hdf5\n",
      "2255/2255 [==============================] - 632s 280ms/step - loss: 1.2915\n",
      "Epoch 33/50\n",
      "2255/2255 [==============================] - ETA: 0s - loss: 1.2814\n",
      "Epoch 00033: loss improved from 1.29148 to 1.28140, saving model to weights-improvement-33-1.2814-bigger.hdf5\n",
      "2255/2255 [==============================] - 636s 282ms/step - loss: 1.2814\n",
      "Epoch 34/50\n",
      "2255/2255 [==============================] - ETA: 0s - loss: 1.2765\n",
      "Epoch 00034: loss improved from 1.28140 to 1.27649, saving model to weights-improvement-34-1.2765-bigger.hdf5\n",
      "2255/2255 [==============================] - 638s 283ms/step - loss: 1.2765\n",
      "Epoch 35/50\n",
      "2255/2255 [==============================] - ETA: 0s - loss: 1.2648\n",
      "Epoch 00035: loss improved from 1.27649 to 1.26477, saving model to weights-improvement-35-1.2648-bigger.hdf5\n",
      "2255/2255 [==============================] - 638s 283ms/step - loss: 1.2648\n",
      "Epoch 36/50\n",
      "2255/2255 [==============================] - ETA: 0s - loss: 1.2612\n",
      "Epoch 00036: loss improved from 1.26477 to 1.26118, saving model to weights-improvement-36-1.2612-bigger.hdf5\n",
      "2255/2255 [==============================] - 635s 282ms/step - loss: 1.2612\n",
      "Epoch 37/50\n",
      "2255/2255 [==============================] - ETA: 0s - loss: 1.2541\n",
      "Epoch 00037: loss improved from 1.26118 to 1.25411, saving model to weights-improvement-37-1.2541-bigger.hdf5\n",
      "2255/2255 [==============================] - 635s 282ms/step - loss: 1.2541\n",
      "Epoch 38/50\n",
      "2255/2255 [==============================] - ETA: 0s - loss: 1.2520\n",
      "Epoch 00038: loss improved from 1.25411 to 1.25200, saving model to weights-improvement-38-1.2520-bigger.hdf5\n",
      "2255/2255 [==============================] - 640s 284ms/step - loss: 1.2520\n",
      "Epoch 39/50\n",
      "2255/2255 [==============================] - ETA: 0s - loss: 1.2429\n",
      "Epoch 00039: loss improved from 1.25200 to 1.24294, saving model to weights-improvement-39-1.2429-bigger.hdf5\n",
      "2255/2255 [==============================] - 628s 279ms/step - loss: 1.2429\n",
      "Epoch 40/50\n",
      "2255/2255 [==============================] - ETA: 0s - loss: 1.2385\n",
      "Epoch 00040: loss improved from 1.24294 to 1.23854, saving model to weights-improvement-40-1.2385-bigger.hdf5\n",
      "2255/2255 [==============================] - 634s 281ms/step - loss: 1.2385\n",
      "Epoch 41/50\n",
      "2255/2255 [==============================] - ETA: 0s - loss: 1.2317\n",
      "Epoch 00041: loss improved from 1.23854 to 1.23167, saving model to weights-improvement-41-1.2317-bigger.hdf5\n",
      "2255/2255 [==============================] - 634s 281ms/step - loss: 1.2317\n",
      "Epoch 42/50\n",
      "2255/2255 [==============================] - ETA: 0s - loss: 1.2277\n",
      "Epoch 00042: loss improved from 1.23167 to 1.22773, saving model to weights-improvement-42-1.2277-bigger.hdf5\n",
      "2255/2255 [==============================] - 631s 280ms/step - loss: 1.2277\n",
      "Epoch 43/50\n",
      "2255/2255 [==============================] - ETA: 0s - loss: 1.2255\n",
      "Epoch 00043: loss improved from 1.22773 to 1.22548, saving model to weights-improvement-43-1.2255-bigger.hdf5\n",
      "2255/2255 [==============================] - 640s 284ms/step - loss: 1.2255\n",
      "Epoch 44/50\n",
      "2255/2255 [==============================] - ETA: 0s - loss: 1.2240\n",
      "Epoch 00044: loss improved from 1.22548 to 1.22398, saving model to weights-improvement-44-1.2240-bigger.hdf5\n",
      "2255/2255 [==============================] - 640s 284ms/step - loss: 1.2240\n",
      "Epoch 45/50\n",
      "2255/2255 [==============================] - ETA: 0s - loss: 1.2169\n",
      "Epoch 00045: loss improved from 1.22398 to 1.21690, saving model to weights-improvement-45-1.2169-bigger.hdf5\n",
      "2255/2255 [==============================] - 640s 284ms/step - loss: 1.2169\n",
      "Epoch 46/50\n",
      "2255/2255 [==============================] - ETA: 0s - loss: 1.2151\n",
      "Epoch 00046: loss improved from 1.21690 to 1.21509, saving model to weights-improvement-46-1.2151-bigger.hdf5\n",
      "2255/2255 [==============================] - 641s 284ms/step - loss: 1.2151\n",
      "Epoch 47/50\n",
      "2255/2255 [==============================] - ETA: 0s - loss: 1.2121\n",
      "Epoch 00047: loss improved from 1.21509 to 1.21209, saving model to weights-improvement-47-1.2121-bigger.hdf5\n",
      "2255/2255 [==============================] - 644s 286ms/step - loss: 1.2121\n",
      "Epoch 48/50\n",
      "2255/2255 [==============================] - ETA: 0s - loss: 1.2081\n",
      "Epoch 00048: loss improved from 1.21209 to 1.20812, saving model to weights-improvement-48-1.2081-bigger.hdf5\n",
      "2255/2255 [==============================] - 627s 278ms/step - loss: 1.2081\n",
      "Epoch 49/50\n",
      "2255/2255 [==============================] - ETA: 0s - loss: 1.2031\n",
      "Epoch 00049: loss improved from 1.20812 to 1.20312, saving model to weights-improvement-49-1.2031-bigger.hdf5\n",
      "2255/2255 [==============================] - 626s 278ms/step - loss: 1.2031\n",
      "Epoch 50/50\n",
      "2255/2255 [==============================] - ETA: 0s - loss: 1.2028\n",
      "Epoch 00050: loss improved from 1.20312 to 1.20280, saving model to weights-improvement-50-1.2028-bigger.hdf5\n",
      "2255/2255 [==============================] - 637s 283ms/step - loss: 1.2028\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f81c034fb90>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "# load ascii text and covert to lowercase\n",
    "filename = \"wonderland.txt\"\n",
    "raw_text = open(filename, 'r', encoding='utf-8').read()\n",
    "raw_text = raw_text.lower()\n",
    "# create mapping of unique chars to integers\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "# summarize the loaded data\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print (\"Total Characters: \", n_chars)\n",
    "print (\"Total Vocab: \", n_vocab)\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "\tseq_in = raw_text[i:i + seq_length]\n",
    "\tseq_out = raw_text[i + seq_length]\n",
    "\tdataX.append([char_to_int[char] for char in seq_in])\n",
    "\tdataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print (\"Total Patterns: \", n_patterns)\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)\n",
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "# define the checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "# fit the model\n",
    "model.fit(X, y, epochs=50, batch_size=64, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\" ly, and the queen was\n",
      "silent.\n",
      "\n",
      "the king laid his hand upon her arm, and timidly said 'consider, my\n",
      "d \"\n",
      "ear, i wish you wouldn't be a lowse--and the sabbit had a little bat it it to sell you coul here?'\n",
      "\n",
      "'i'd rather not,' said the caterpillar.\n",
      "\n",
      "'well, i've sried to say \" sie said to herself, 'they don't tee some minutes that makes them bone,'\n",
      "\n",
      "'i don't know it was your taid,' said the caterpillar.\n",
      "\n",
      "'well, it surne the sea, she much surtle sime the had not ro done the thing as the court, and the sable sealy ruite all the white rabbit say and san and steeenly as the court, and the three gardeners its louth and said, 'it she sand the thing the white rabbit sald to herself, and she thought to her hn the sime it would be a lowse that she was not a minute or two, and the three gardeners instantly was the fatthr, and then they was not a mittle beaodererl shat it was the first right into the sable for the wood, 'if you don't lnow the mittle door better now,'\n",
      "\n",
      "'i'd nadr wo het to the shate is in the sea, shouted the queen, that it say the things better now.'\n",
      "\n",
      "'i dan't be a gardy tomesh!' said the\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# load the network weights\n",
    "filename = \"weights-improvement-50-1.2028-bigger.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "# pick a random seed\n",
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print (\"Seed:\")\n",
    "print (\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "# generate characters\n",
    "for i in range(1000):\n",
    "\tx = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "\tx = x / float(n_vocab)\n",
    "\tprediction = model.predict(x, verbose=0)\n",
    "\tindex = numpy.argmax(prediction)\n",
    "\tresult = int_to_char[index]\n",
    "\tseq_in = [int_to_char[value] for value in pattern]\n",
    "\tsys.stdout.write(result)\n",
    "\tpattern.append(index)\n",
    "\tpattern = pattern[1:len(pattern)]\n",
    "print (\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\" ne of the guinea-pigs cheered, and was immediately suppressed by\n",
      "the officers of the court. (as that \"\n",
      " i had as it mane you don't be a dauchtly wask to her to in that '\n",
      "\n",
      "'i wish you wouldn't hise your hands,' said the king.\n",
      "\n",
      "'it was the soiderens said with the bantte,' said the caterpillar.\n",
      "\n",
      "'well, i've sried to say \" sie said to herself, 'they don't tee some minutes that makes them bone,'\n",
      "\n",
      "'i don't know it was your taid,' said the caterpillar.\n",
      "\n",
      "'well, it surne the sea, she much surtle sime the had not ro done the thing as the court, and the sable sealy ruite all the white rabbit say and san and steeenly as the court, and the three gardeners its louth and said, 'it she sand the thing the white rabbit sald to herself, and she thought to her hn the sime it would be a lowse that she was not a minute or two, and the three gardeners instantly was the fatthr, and then they was not a mittle beaodererl shat it was the first right into the sable for the wood, 'if you don't lnow the mittle door better now,'\n",
      "\n",
      "'i'd nadr wo het to the shate is in the sea, shouted the queen, that it say the things b\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# load the network weights\n",
    "filename = \"weights-improvement-50-1.2028-bigger.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "# pick a random seed\n",
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print (\"Seed:\")\n",
    "print (\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "# generate characters\n",
    "for i in range(1000):\n",
    "\tx = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "\tx = x / float(n_vocab)\n",
    "\tprediction = model.predict(x, verbose=0)\n",
    "\tindex = numpy.argmax(prediction)\n",
    "\tresult = int_to_char[index]\n",
    "\tseq_in = [int_to_char[value] for value in pattern]\n",
    "\tsys.stdout.write(result)\n",
    "\tpattern.append(index)\n",
    "\tpattern = pattern[1:len(pattern)]\n",
    "print (\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
